{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 9 Project\n",
        "\n",
        "This week's project will build an email spam classifier based on the Enron email data set.\n",
        "You will perform your own feature extraction, and use naive Bayes to estimate the probability that a particular email is spam or not.\n",
        "Finally, you will review the tradeoffs from different thresholds for automatically sending emails to the junk folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBdILvlviZs2"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 9 Materials](https://github.com/bu-cds-dx704/dx704-project-09).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRDlXZBVd2aR"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf1nEl0_Khm_"
      },
      "source": [
        "## Part 1: Download Data Set\n",
        "\n",
        "We will be using the Enron spam data set as prepared in this GitHub repository.\n",
        "\n",
        "https://github.com/MWiechmann/enron_spam_data\n",
        "\n",
        "You may need to download this differently depending on your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwpoSzUxKmG9",
        "outputId": "3ace62f1-c32a-462a-d538-36f3638b7dc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-11-03 04:54:49--  https://github.com/MWiechmann/enron_spam_data/raw/refs/heads/master/enron_spam_data.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/MWiechmann/enron_spam_data/refs/heads/master/enron_spam_data.zip [following]\n",
            "--2025-11-03 04:54:49--  https://raw.githubusercontent.com/MWiechmann/enron_spam_data/refs/heads/master/enron_spam_data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15642124 (15M) [application/zip]\n",
            "Saving to: ‘enron_spam_data.zip’\n",
            "\n",
            "enron_spam_data.zip 100%[===================>]  14.92M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-11-03 04:54:50 (123 MB/s) - ‘enron_spam_data.zip’ saved [15642124/15642124]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/MWiechmann/enron_spam_data/raw/refs/heads/master/enron_spam_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EfCir3ILLv8z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "9gn-4hUzLywO",
        "outputId": "f810652e-7829-44dd-e842-137281f53be2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Message ID</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Message</th>\n",
              "      <th>Spam/Ham</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>christmas tree farm pictures</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>vastar resources , inc .</td>\n",
              "      <td>gary , production from the high island larger ...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>calpine daily gas nomination</td>\n",
              "      <td>- calpine daily gas nomination 1 . doc</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>re : issue</td>\n",
              "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>meter 7268 nov allocation</td>\n",
              "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33711</th>\n",
              "      <td>33711</td>\n",
              "      <td>= ? iso - 8859 - 1 ? q ? good _ news _ c = eda...</td>\n",
              "      <td>hello , welcome to gigapharm onlinne shop .\\np...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33712</th>\n",
              "      <td>33712</td>\n",
              "      <td>all prescript medicines are on special . to be...</td>\n",
              "      <td>i got it earlier than expected and it was wrap...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33713</th>\n",
              "      <td>33713</td>\n",
              "      <td>the next generation online pharmacy .</td>\n",
              "      <td>are you ready to rock on ? let the man in you ...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33714</th>\n",
              "      <td>33714</td>\n",
              "      <td>bloow in 5 - 10 times the time</td>\n",
              "      <td>learn how to last 5 - 10 times longer in\\nbed ...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33715</th>\n",
              "      <td>33715</td>\n",
              "      <td>dear sir , i am interested in it</td>\n",
              "      <td>hi : )\\ndo you need some softwares ? i can giv...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>33716 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Message ID                                            Subject  \\\n",
              "0               0                       christmas tree farm pictures   \n",
              "1               1                           vastar resources , inc .   \n",
              "2               2                       calpine daily gas nomination   \n",
              "3               3                                         re : issue   \n",
              "4               4                          meter 7268 nov allocation   \n",
              "...           ...                                                ...   \n",
              "33711       33711  = ? iso - 8859 - 1 ? q ? good _ news _ c = eda...   \n",
              "33712       33712  all prescript medicines are on special . to be...   \n",
              "33713       33713              the next generation online pharmacy .   \n",
              "33714       33714                     bloow in 5 - 10 times the time   \n",
              "33715       33715                   dear sir , i am interested in it   \n",
              "\n",
              "                                                 Message Spam/Ham        Date  \n",
              "0                                                    NaN      ham  1999-12-10  \n",
              "1      gary , production from the high island larger ...      ham  1999-12-13  \n",
              "2                 - calpine daily gas nomination 1 . doc      ham  1999-12-14  \n",
              "3      fyi - see note below - already done .\\nstella\\...      ham  1999-12-14  \n",
              "4      fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham  1999-12-14  \n",
              "...                                                  ...      ...         ...  \n",
              "33711  hello , welcome to gigapharm onlinne shop .\\np...     spam  2005-07-29  \n",
              "33712  i got it earlier than expected and it was wrap...     spam  2005-07-29  \n",
              "33713  are you ready to rock on ? let the man in you ...     spam  2005-07-30  \n",
              "33714  learn how to last 5 - 10 times longer in\\nbed ...     spam  2005-07-30  \n",
              "33715  hi : )\\ndo you need some softwares ? i can giv...     spam  2005-07-31  \n",
              "\n",
              "[33716 rows x 5 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pandas can read the zip file directly\n",
        "enron_spam_data = pd.read_csv(\"enron_spam_data.zip\")\n",
        "enron_spam_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYypb_fJWF_A",
        "outputId": "17478b00-1c10-4026-a42b-dcc8a368eab5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.5092834262664611)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(enron_spam_data[\"Spam/Ham\"] == \"spam\").mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 2: Design a Feature Extractor\n",
        "\n",
        "Design a feature extractor for this data set and write out two files of features based on the text.\n",
        "Don't forget that both the Subject and Message columns are relevant sources of text data.\n",
        "For each email, you should count the number of repetitions of each feature present.\n",
        "The auto-grader will assume that you are using a multinomial distribution in the following problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-CF6wtn_VRjp"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "try:\n",
        "    _df = enron_spam_data.copy()\n",
        "except NameError:\n",
        "    _df = pd.read_csv(\"enron_spam_data.zip\")\n",
        "\n",
        "required_cols = {\"Message ID\", \"Subject\", \"Message\"}\n",
        "missing = required_cols - set(_df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "\n",
        "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
        "EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", re.IGNORECASE)\n",
        "NUMBER_RE = re.compile(r\"\\b\\d+(?:[\\d,./:-]*\\d)?\\b\")\n",
        "\n",
        "TOKEN_RE = re.compile(r\"\\b[\\w']+\\b\", re.UNICODE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    t = text.lower()\n",
        "    t = URL_RE.sub(\"__URL__\", t)\n",
        "    t = EMAIL_RE.sub(\"__EMAIL__\", t)\n",
        "    t = NUMBER_RE.sub(\"__NUMBER__\", t)\n",
        "    return t\n",
        "\n",
        "def tokenize(text: str):\n",
        "    # returns a list of tokens\n",
        "    return TOKEN_RE.findall(text)\n",
        "\n",
        "def extract_features_from_row(row) -> dict:\n",
        "    # Combine Subject and Message; both are relevant\n",
        "    subject = normalize_text(row.get(\"Subject\", \"\"))\n",
        "    message = normalize_text(row.get(\"Message\", \"\"))\n",
        "\n",
        "    # Simple unigram bag-of-words with counts (multinomial NB friendly)\n",
        "    tokens = []\n",
        "    if subject:\n",
        "        tokens.extend(tokenize(subject))\n",
        "    if message:\n",
        "        tokens.extend(tokenize(message))\n",
        "\n",
        "    counts = Counter(tokens)\n",
        "\n",
        "    # Optionally include a few useful meta-features (still counts, so multinomial-compatible)\n",
        "    # These can help NB without complicating the grader’s expectations.\n",
        "    # e.g., presence counts for URL, EMAIL, NUMBER placeholders\n",
        "    if \"__URL__\" in counts:\n",
        "        counts[\"HAS_URL\"] = counts[\"__URL__\"]\n",
        "    if \"__EMAIL__\" in counts:\n",
        "        counts[\"HAS_EMAIL\"] = counts[\"__EMAIL__\"]\n",
        "    if \"__NUMBER__\" in counts:\n",
        "        counts[\"HAS_NUMBER\"] = counts[\"__NUMBER__\"]\n",
        "\n",
        "    # Convert Counter to a plain dict of str -> int; keep only positive counts\n",
        "    features = {str(k): int(v) for k, v in counts.items() if v > 0}\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_records = []\n",
        "for _, row in _df.iterrows():\n",
        "    mid = int(row[\"Message ID\"])\n",
        "    feats = extract_features_from_row(row)\n",
        "    features_records.append({\n",
        "        \"Message ID\": mid,\n",
        "        \"features_json\": json.dumps(feats, sort_keys=True, ensure_ascii=False)\n",
        "    })\n",
        "\n",
        "features_df = pd.DataFrame(features_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g90ug-qYVWI2"
      },
      "source": [
        "Assign a row to the test data set if `Message ID % 30 == 0` and assign it to the training data set otherwise.\n",
        "Write two files, \"train-features.tsv\" and \"test-features.tsv\" with two columns, Message ID and features_json.\n",
        "The features_json column should contain a JSON dictionary where the keys are your feature names and the values are integer feature values.\n",
        "This will give us a sparse feature representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t7AjXVlXUpaR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total emails: 33716\n",
            "Train rows : 32592   -> train-features.tsv\n",
            "Test rows  : 1124    -> test-features.tsv\n",
            "\n",
            "Sample rows:\n",
            " Message ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     features_json\n",
            "          1 {\"HAS_NUMBER\": 149, \"__NUMBER__\": 149, \"a\": 6, \"about\": 1, \"advises\": 1, \"am\": 6, \"america\": 1, \"and\": 4, \"appears\": 1, \"are\": 1, \"as\": 1, \"at\": 3, \"attached\": 1, \"based\": 1, \"be\": 1, \"between\": 1, \"bill\": 1, \"block\": 2, \"bmar\": 1, \"bryan\": 1, \"by\": 3, \"call\": 1, \"can\": 1, \"carlos\": 3, \"cc\": 3, \"coming\": 1, \"commence\": 1, \"commenced\": 1, \"contacts\": 1, \"control\": 1, \"conversations\": 1, \"corp\": 1, \"d\": 3, \"daren\": 3, \"darren\": 1, \"day\": 1, \"e\": 1, \"each\": 1, \"ect\": 17, \"effective\": 1, \"enron\": 1, \"erroneously\": 1, \"estimate\": 1, \"everything\": 1, \"expects\": 2, \"farmer\": 3, \"fax\": 2, \"fischer\": 1, \"following\": 1, \"for\": 3, \"forwarded\": 3, \"from\": 3, \"ftp\": 1, \"future\": 1, \"gary\": 2, \"gas\": 1, \"george\": 7, \"get\": 2, \"going\": 1, \"graves\": 2, \"gross\": 3, \"harris\": 2, \"her\": 2, \"hi\": 1, \"high\": 2, \"hou\": 10, \"hours\": 17, \"how\": 1, \"i\": 2, \"in\": 1, \"inc\": 4, \"increase\": 1, \"island\": 2, \"j\": 4, \"larger\": 2, \"linda\": 5, \"m\": 2, \"mail\": 1, \"melissa\": 2, \"min\": 1, \"mscf\": 1, \"my\": 1, \"nomination\": 1, \"nominations\": 1, \"north\": 1, \"notification\": 1, \"now\": 1, \"number\": 1, \"numbers\": 1, \"of\": 2, \"on\": 7, \"or\": 1, \"owns\": 1, \"p\": 1, \"please\": 2, \"previously\": 1, \"production\": 3, \"provide\": 1, \"record\": 1, \"referred\": 1, \"resources\": 4, \"rodriguez\": 1, \"s\": 1, \"saturday\": 1, \"see\": 1, \"set\": 1, \"she\": 1, \"so\": 1, \"someone\": 1, \"sometime\": 1, \"subject\": 3, \"submit\": 1, \"telephone\": 1, \"thanks\": 1, \"that\": 2, \"the\": 8, \"time\": 1, \"to\": 8, \"told\": 1, \"tomorrow\": 4, \"turn\": 1, \"up\": 2, \"vastar\": 6, \"via\": 1, \"voice\": 2, \"we\": 1, \"weissman\": 5, \"well\": 2, \"with\": 2, \"would\": 1, \"x\": 2, \"you\": 1}\n",
            "          2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {\"HAS_NUMBER\": 1, \"__NUMBER__\": 1, \"calpine\": 2, \"daily\": 2, \"doc\": 1, \"gas\": 2, \"nomination\": 2}\n",
            " Message ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 features_json\n",
            "          0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {\"christmas\": 1, \"farm\": 1, \"pictures\": 1, \"tree\": 1}\n",
            "         30 {\"HAS_NUMBER\": 13, \"__NUMBER__\": 13, \"a\": 1, \"all\": 1, \"am\": 1, \"and\": 5, \"any\": 3, \"at\": 1, \"attached\": 2, \"be\": 1, \"before\": 1, \"both\": 1, \"by\": 2, \"calls\": 1, \"carlos\": 3, \"cc\": 1, \"changed\": 1, \"changes\": 1, \"christmas\": 1, \"complete\": 1, \"confident\": 1, \"costilla\": 1, \"customer\": 1, \"daren\": 2, \"did\": 1, \"ect\": 5, \"estimate\": 5, \"except\": 1, \"fax\": 1, \"feel\": 1, \"file\": 3, \"final\": 1, \"for\": 1, \"forwarded\": 1, \"from\": 1, \"good\": 1, \"graves\": 1, \"have\": 4, \"hopefully\": 1, \"hou\": 3, \"however\": 1, \"i\": 3, \"if\": 3, \"in\": 1, \"is\": 2, \"it\": 2, \"january\": 3, \"know\": 1, \"l\": 1, \"last\": 1, \"let\": 1, \"made\": 1, \"me\": 1, \"melissa\": 1, \"merry\": 1, \"monday\": 2, \"more\": 1, \"much\": 1, \"nom\": 1, \"nominations\": 1, \"noms\": 1, \"noon\": 1, \"not\": 3, \"number\": 1, \"of\": 1, \"on\": 1, \"once\": 1, \"please\": 3, \"pm\": 1, \"pretty\": 2, \"production\": 3, \"provided\": 1, \"questions\": 1, \"receive\": 2, \"reviewed\": 1, \"s\": 1, \"said\": 1, \"scs\": 1, \"see\": 1, \"send\": 1, \"should\": 2, \"smith\": 2, \"subject\": 1, \"superior\": 1, \"susan\": 3, \"taylor\": 1, \"thanks\": 2, \"that\": 3, \"the\": 6, \"their\": 2, \"therefore\": 1, \"they\": 2, \"this\": 3, \"to\": 5, \"traders\": 1, \"update\": 1, \"vance\": 1, \"vlt\": 1, \"walter\": 1, \"was\": 2, \"week\": 1, \"what\": 1, \"will\": 1, \"would\": 1, \"x\": 2, \"you\": 2, \"your\": 1, \"yours\": 1}\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Split per the rule: test if Message ID % 30 == 0, else train\n",
        "is_test = features_df[\"Message ID\"] % 30 == 0\n",
        "train_df = features_df.loc[~is_test].copy()\n",
        "test_df  = features_df.loc[ is_test].copy()\n",
        "\n",
        "# Save to TSV\n",
        "train_df.to_csv(\"train-features.tsv\", sep=\"\\t\", index=False)\n",
        "test_df.to_csv(\"test-features.tsv\",  sep=\"\\t\", index=False)\n",
        "\n",
        "# Quick report\n",
        "print(f\"Total emails: {len(features_df)}\")\n",
        "print(f\"Train rows : {len(train_df)}   -> train-features.tsv\")\n",
        "print(f\"Test rows  : {len(test_df)}    -> test-features.tsv\")\n",
        "\n",
        "print(\"\\nSample rows:\")\n",
        "print(train_df.head(2).to_string(index=False))\n",
        "print(test_df.head(2).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAEYBd7WUrC0"
      },
      "source": [
        "Submit \"train-features.tsv\" and \"test-features.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwrLU0aIaNB7"
      },
      "source": [
        "Hint: these features will be graded based on the test accuracy of a logistic regression based on the training features.\n",
        "This is to make sure that your feature set is not degenerate; you do not need to compute this regression yourself.\n",
        "You can separately assess your feature quality based on your results in part 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_PhU4d5vEFX"
      },
      "source": [
        "## Part 3: Compute Conditional Probabilities\n",
        "\n",
        "Based on your training data, compute appropriate conditional probabilities for use with naïve Bayes.\n",
        "Use of additive smoothing with $\\alpha=1$ to avoid zeros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3MKi6er-Vde4"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import json\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(\"train-features.tsv\", sep=\"\\t\")\n",
        "test_df  = pd.read_csv(\"test-features.tsv\", sep=\"\\t\")\n",
        "\n",
        "train_df[\"features_dict\"] = train_df[\"features_json\"].apply(json.loads)\n",
        "test_df[\"features_dict\"]  = test_df[\"features_json\"].apply(json.loads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using label column: 'Spam/Ham'\n",
            "label\n",
            "spam(1)    17171\n",
            "ham(0)     16545\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def _load_original_df():\n",
        "    # Prefer in-memory if earlier cells created it\n",
        "    try:\n",
        "        return enron_spam_data.copy()\n",
        "    except NameError:\n",
        "        pass\n",
        "    # Try common filenames; pandas can read a single-csv zip\n",
        "    for cand in [\"enron_spam_data.zip\", \"enron_spam_data.csv\", \"enron.csv\", \"data.csv\"]:\n",
        "        try:\n",
        "            return pd.read_csv(cand)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "raw_df = _load_original_df()\n",
        "if raw_df is None:\n",
        "    raise ValueError(\n",
        "        \"Could not load the original Enron dataset to obtain labels. \"\n",
        "        \"Expected one of: enron_spam_data.zip / enron_spam_data.csv\"\n",
        "    )\n",
        "\n",
        "# Normalize/locate Message ID\n",
        "if \"Message ID\" not in raw_df.columns:\n",
        "    for alt in [\"MessageID\", \"message_id\", \"msg_id\", \"Id\", \"id\"]:\n",
        "        if alt in raw_df.columns:\n",
        "            raw_df[\"Message ID\"] = raw_df[alt]\n",
        "            break\n",
        "if \"Message ID\" not in raw_df.columns:\n",
        "    raise ValueError(\"Could not find a 'Message ID' column in the original dataset.\")\n",
        "\n",
        "def _to01(x):\n",
        "    if isinstance(x, str):\n",
        "        xs = x.strip().lower()\n",
        "        if xs in {\"spam\", \"s\", \"1\", \"true\", \"yes\", \"y\"}:\n",
        "            return 1\n",
        "        if xs in {\"ham\", \"h\", \"0\", \"false\", \"no\", \"n\", \"not_spam\", \"not spam\"}:\n",
        "            return 0\n",
        "    if isinstance(x, (bool, np.bool_)):\n",
        "        return 1 if x else 0\n",
        "    try:\n",
        "        v = int(x)\n",
        "        return 1 if v != 0 else 0\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def _get_labels_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1) Try many common label names first\n",
        "    preferred = [\n",
        "        \"Spam\", \"spam\", \"Label\", \"label\", \"Class\", \"class\", \"Target\", \"target\",\n",
        "        \"is_spam\", \"Is Spam\", \"Spam/Ham\", \"SpamHam\", \"Category\", \"category\"\n",
        "    ]\n",
        "    cand = None\n",
        "    for c in preferred:\n",
        "        if c in df.columns:\n",
        "            cand = c\n",
        "            break\n",
        "\n",
        "    # 2) If not found, auto-detect a binary-like column (exactly 2 unique non-null values)\n",
        "    if cand is None:\n",
        "        for c in df.columns:\n",
        "            if c == \"Message ID\":\n",
        "                continue\n",
        "            vals = pd.Series(df[c]).dropna().unique()\n",
        "            if len(vals) == 2:\n",
        "                cand = c\n",
        "                break\n",
        "\n",
        "    if cand is None:\n",
        "        raise ValueError(\n",
        "            f\"No label column found. Available columns: {list(df.columns)}. \"\n",
        "            \"Expected something like 'Spam', 'Label', or a binary-like column.\"\n",
        "        )\n",
        "\n",
        "    mapped = df[cand].apply(_to01)\n",
        "\n",
        "    # If mapping failed (all NaN), try a heuristic using the two unique values\n",
        "    if mapped.isna().all():\n",
        "        uniq = pd.Series(df[cand]).dropna().unique()\n",
        "        if len(uniq) == 2:\n",
        "            u0, u1 = uniq[0], uniq[1]\n",
        "            s0, s1 = str(u0).lower(), str(u1).lower()\n",
        "            spam_val = u0 if \"spam\" in s0 else (u1 if \"spam\" in s1 else u1)\n",
        "            ham_val  = u1 if spam_val is u0 else u0\n",
        "            mapped = df[cand].apply(lambda v: 1 if v == spam_val else (0 if v == ham_val else np.nan))\n",
        "        else:\n",
        "            raise ValueError(f\"Could not coerce label column '{cand}' to binary 0/1.\")\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"Message ID\": df[\"Message ID\"].astype(int),\n",
        "        \"label\": mapped.astype(int)\n",
        "    })\n",
        "    print(f\"Using label column: '{cand}'\")\n",
        "    print(out[\"label\"].value_counts(dropna=False).rename(index={0: \"ham(0)\", 1: \"spam(1)\"}))\n",
        "    return out\n",
        "\n",
        "labels_df = _get_labels_df(raw_df)\n",
        "\n",
        "# Merge labels with features\n",
        "train = train_df.merge(labels_df, on=\"Message ID\", how=\"inner\")\n",
        "test  = test_df.merge(labels_df,  on=\"Message ID\", how=\"left\")\n",
        "\n",
        "if train[\"label\"].isna().any():\n",
        "    raise ValueError(\"Some training rows have missing labels after merge; check 'Message ID' alignment.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Multinomial Naive Bayes (manual; to get P(feature|class))\n",
        "alpha = 1.0  # Laplace smoothing\n",
        "\n",
        "# Class-separated token counts\n",
        "token_counts: dict[int, Counter] = {0: Counter(), 1: Counter()}\n",
        "total_counts: dict[int, int]     = {0: 0, 1: 0}\n",
        "class_docs: dict[int, int]       = {0: 0, 1: 0}\n",
        "\n",
        "for _, row in train.iterrows():\n",
        "    y = int(row[\"label\"])\n",
        "    feats = row[\"features_dict\"]\n",
        "    class_docs[y] += 1\n",
        "    for tok, cnt in feats.items():\n",
        "        cnt = int(cnt)\n",
        "        if cnt <= 0: \n",
        "            continue\n",
        "        token_counts[y][tok] += cnt\n",
        "        total_counts[y] += cnt\n",
        "\n",
        "# Vocabulary and conditional probs\n",
        "vocab = set(token_counts[0].keys()) | set(token_counts[1].keys())\n",
        "V = len(vocab)\n",
        "\n",
        "ham_prob = {}\n",
        "spam_prob = {}\n",
        "denom_ham  = total_counts[0] + alpha * V\n",
        "denom_spam = total_counts[1] + alpha * V\n",
        "for tok in vocab:\n",
        "    ham_prob[tok]  = (token_counts[0][tok] + alpha) / denom_ham\n",
        "    spam_prob[tok] = (token_counts[1][tok] + alpha) / denom_spam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbDJfLCdVfHh"
      },
      "source": [
        "Save the conditional probabilities in a file \"feature-probabilities.tsv\" with columns feature, ham_probability and spam_probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kTVFW327VsOC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 142247 features -> feature-probabilities.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "prob_df = pd.DataFrame({\n",
        "    \"feature\": list(vocab),\n",
        "    \"ham_probability\":  [ham_prob[t]  for t in vocab],\n",
        "    \"spam_probability\": [spam_prob[t] for t in vocab],\n",
        "})\n",
        "prob_df.sort_values(\"feature\").to_csv(\"feature-probabilities.tsv\", sep=\"\\t\", index=False)\n",
        "print(f\"Saved {len(prob_df)} features -> feature-probabilities.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip-k6K-hVt6q"
      },
      "source": [
        "Submit \"feature-probabilities.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuQpZbILYqNd"
      },
      "source": [
        "## Part 4: Implement a Naïve Bayes Classifier\n",
        "\n",
        "Implement a naïve Bayes classifier based on your previous feature probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jkZeyZgsWr5-"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "probs_df = pd.read_csv(\"feature-probabilities.tsv\", sep=\"\\t\")\n",
        "test_df  = pd.read_csv(\"train-features.tsv\", sep=\"\\t\")\n",
        "\n",
        "def _parse_json(s):\n",
        "    try:\n",
        "        d = json.loads(s)\n",
        "        return {str(k): int(v) for k, v in d.items()}\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "test_df[\"features_dict\"] = test_df[\"features_json\"].apply(_parse_json)\n",
        "\n",
        "# Put probabilities into dicts (vocab from Part 3)\n",
        "ham_prob  = dict(zip(probs_df[\"feature\"], probs_df[\"ham_probability\"]))\n",
        "spam_prob = dict(zip(probs_df[\"feature\"], probs_df[\"spam_probability\"]))\n",
        "vocab = set(ham_prob.keys()) | set(spam_prob.keys())\n",
        "\n",
        "# For numerical stability use logs\n",
        "log_ham  = {t: math.log(p) for t, p in ham_prob.items()}\n",
        "log_spam = {t: math.log(p) for t, p in spam_prob.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Priors (from TRAIN split): P(ham)=0.4907, P(spam)=0.5093\n"
          ]
        }
      ],
      "source": [
        "def _load_original_df():\n",
        "    try:\n",
        "        return enron_spam_data.copy()\n",
        "    except NameError:\n",
        "        pass\n",
        "    for cand in [\"enron_spam_data.zip\", \"enron_spam_data.csv\", \"enron.csv\", \"data.csv\"]:\n",
        "        try:\n",
        "            return pd.read_csv(cand)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "raw_df = _load_original_df()\n",
        "if raw_df is None:\n",
        "    raise ValueError(\"Could not load the original dataset to compute priors.\")\n",
        "\n",
        "# Normalize Message ID\n",
        "if \"Message ID\" not in raw_df.columns:\n",
        "    for alt in [\"MessageID\", \"message_id\", \"msg_id\", \"Id\", \"id\"]:\n",
        "        if alt in raw_df.columns:\n",
        "            raw_df[\"Message ID\"] = raw_df[alt]\n",
        "            break\n",
        "if \"Message ID\" not in raw_df.columns:\n",
        "    raise ValueError(\"No 'Message ID' column found.\")\n",
        "\n",
        "# Robust label mapping -> {0=ham,1=spam}\n",
        "def _to01(x):\n",
        "    if isinstance(x, str):\n",
        "        xs = x.strip().lower()\n",
        "        if xs in {\"spam\",\"s\",\"1\",\"true\",\"yes\",\"y\"}: return 1\n",
        "        if xs in {\"ham\",\"h\",\"0\",\"false\",\"no\",\"n\",\"not_spam\",\"not spam\"}: return 0\n",
        "    if isinstance(x, (bool, np.bool_)): return 1 if x else 0\n",
        "    try:\n",
        "        v = int(x); return 1 if v != 0 else 0\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def _get_labels_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    preferred = [\n",
        "        \"Spam\",\"spam\",\"Label\",\"label\",\"Class\",\"class\",\"Target\",\"target\",\n",
        "        \"is_spam\",\"Is Spam\",\"Spam/Ham\",\"SpamHam\",\"Category\",\"category\"\n",
        "    ]\n",
        "    cand = None\n",
        "    for c in preferred:\n",
        "        if c in df.columns:\n",
        "            cand = c; break\n",
        "    if cand is None:\n",
        "        # auto-detect binary-like column\n",
        "        for c in df.columns:\n",
        "            if c == \"Message ID\": continue\n",
        "            vals = pd.Series(df[c]).dropna().unique()\n",
        "            if len(vals) == 2:\n",
        "                cand = c; break\n",
        "    if cand is None:\n",
        "        raise ValueError(\"No label column found for computing priors.\")\n",
        "    mapped = df[cand].apply(_to01)\n",
        "    return pd.DataFrame({\"Message ID\": df[\"Message ID\"].astype(int), \"label\": mapped.astype(int)})\n",
        "\n",
        "labels_df = _get_labels_df(raw_df)\n",
        "\n",
        "# Use TRAIN split (Message ID % 30 != 0) for priors\n",
        "train_mask = labels_df[\"Message ID\"] % 30 != 0\n",
        "train_labels = labels_df.loc[train_mask].copy()\n",
        "n_docs = len(train_labels)\n",
        "n_ham  = (train_labels[\"label\"] == 0).sum()\n",
        "n_spam = (train_labels[\"label\"] == 1).sum()\n",
        "\n",
        "# Laplace-smoothed priors\n",
        "P_ham  = (n_ham + 1) / (n_docs + 2)\n",
        "P_spam = (n_spam + 1) / (n_docs + 2)\n",
        "log_prior_ham  = math.log(P_ham)\n",
        "log_prior_spam = math.log(P_spam)\n",
        "\n",
        "print(f\"Priors (from TRAIN split): P(ham)={P_ham:.4f}, P(spam)={P_spam:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score train rows -> posterior P(ham|msg), P(spam|msg)\n",
        "def _posterior_probs(feats: dict[str, int]) -> tuple[float, float]:\n",
        "    lh = log_prior_ham\n",
        "    ls = log_prior_spam\n",
        "    for tok, cnt in feats.items():\n",
        "        if cnt <= 0: \n",
        "            continue\n",
        "        if tok in vocab:\n",
        "            lh += cnt * log_ham[tok]\n",
        "            ls += cnt * log_spam[tok]\n",
        "        # OOV tokens ignored (same constant across classes if smoothed)\n",
        "    # Normalize\n",
        "    m = max(lh, ls)\n",
        "    ph = math.exp(lh - m)\n",
        "    ps = math.exp(ls - m)\n",
        "    z = ph + ps\n",
        "    return ph / z, ps / z\n",
        "\n",
        "ham_post, spam_post = [], []\n",
        "for feats in train_df[\"features_dict\"]:\n",
        "    ph, ps = _posterior_probs(feats)\n",
        "    ham_post.append(ph)\n",
        "    spam_post.append(ps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeYGfCYXW89l"
      },
      "source": [
        "Save your prediction probabilities to \"train-predictions.tsv\" with columns Message ID, ham and spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kCKrHbpqZ1gY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved -> train-predictions.tsv\n",
            " message id  ham          spam\n",
            "          1  1.0 1.715648e-199\n",
            "          2  1.0  1.012723e-12\n",
            "          3  1.0 1.223237e-153\n",
            "          4  1.0 5.580284e-147\n",
            "          5  1.0  1.607455e-39\n",
            "          6  1.0  9.107797e-22\n",
            "          7  1.0 7.255169e-206\n",
            "          8  1.0  7.284969e-89\n",
            "          9  1.0 8.985004e-246\n",
            "         10  1.0  1.408989e-67\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "out = pd.DataFrame({\n",
        "    \"message id\": train_df[\"Message ID\"].astype(int),\n",
        "    \"ham\": ham_post,\n",
        "    \"spam\": spam_post\n",
        "})\n",
        "out.to_csv(\"train-predictions.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(\"Saved -> train-predictions.tsv\")\n",
        "print(out.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGHYjWN9Z3Sq"
      },
      "source": [
        "Submit \"train-predictions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpTlyFLDOCDj"
      },
      "source": [
        "## Part 5: Predict Spam Probability for Test Data\n",
        "\n",
        "Use your previous classifier to predict spam probability for the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "UELHs9CzXaz1"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "probs_df = pd.read_csv(\"feature-probabilities.tsv\", sep=\"\\t\")  # from Part 3\n",
        "test_df  = pd.read_csv(\"test-features.tsv\", sep=\"\\t\")          # from Part 2\n",
        "\n",
        "def _parse_json(s):\n",
        "    try:\n",
        "        d = json.loads(s)\n",
        "        return {str(k): int(v) for k, v in d.items()}\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "test_df[\"features_dict\"] = test_df[\"features_json\"].apply(_parse_json)\n",
        "\n",
        "# Conditional probabilities learned in Part 3\n",
        "ham_prob  = dict(zip(probs_df[\"feature\"], probs_df[\"ham_probability\"]))\n",
        "spam_prob = dict(zip(probs_df[\"feature\"], probs_df[\"spam_probability\"]))\n",
        "vocab = set(ham_prob.keys()) | set(spam_prob.keys())\n",
        "\n",
        "# Precompute logs for stability\n",
        "log_ham  = {t: math.log(p) for t, p in ham_prob.items()}\n",
        "log_spam = {t: math.log(p) for t, p in spam_prob.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Priors (from TRAIN split): P(ham)=0.4907, P(spam)=0.5093\n"
          ]
        }
      ],
      "source": [
        "def _load_original_df():\n",
        "    # Prefer in-memory if it exists, else try common filenames\n",
        "    try:\n",
        "        return enron_spam_data.copy()\n",
        "    except NameError:\n",
        "        pass\n",
        "    for cand in [\"enron_spam_data.zip\", \"enron_spam_data.csv\", \"enron.csv\", \"data.csv\"]:\n",
        "        try:\n",
        "            return pd.read_csv(cand)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "raw_df = _load_original_df()\n",
        "if raw_df is None:\n",
        "    raise ValueError(\"Could not load the original dataset to compute class priors.\")\n",
        "\n",
        "# Normalize Message ID\n",
        "if \"Message ID\" not in raw_df.columns:\n",
        "    for alt in [\"MessageID\", \"message_id\", \"msg_id\", \"Id\", \"id\"]:\n",
        "        if alt in raw_df.columns:\n",
        "            raw_df[\"Message ID\"] = raw_df[alt]\n",
        "            break\n",
        "if \"Message ID\" not in raw_df.columns:\n",
        "    raise ValueError(\"No 'Message ID' column found for priors computation.\")\n",
        "\n",
        "# Robust label detection -> {0=ham, 1=spam}\n",
        "def _to01(x):\n",
        "    if isinstance(x, str):\n",
        "        xs = x.strip().lower()\n",
        "        if xs in {\"spam\",\"s\",\"1\",\"true\",\"yes\",\"y\"}: return 1\n",
        "        if xs in {\"ham\",\"h\",\"0\",\"false\",\"no\",\"n\",\"not_spam\",\"not spam\"}: return 0\n",
        "    if isinstance(x, (bool, np.bool_)): return 1 if x else 0\n",
        "    try:\n",
        "        v = int(x); return 1 if v != 0 else 0\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def _get_labels_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    preferred = [\n",
        "        \"Spam\",\"spam\",\"Label\",\"label\",\"Class\",\"class\",\"Target\",\"target\",\n",
        "        \"is_spam\",\"Is Spam\",\"Spam/Ham\",\"SpamHam\",\"Category\",\"category\"\n",
        "    ]\n",
        "    cand = next((c for c in preferred if c in df.columns), None)\n",
        "    if cand is None:\n",
        "        # auto-detect a binary-like column\n",
        "        for c in df.columns:\n",
        "            if c == \"Message ID\": continue\n",
        "            vals = pd.Series(df[c]).dropna().unique()\n",
        "            if len(vals) == 2:\n",
        "                cand = c; break\n",
        "    if cand is None:\n",
        "        raise ValueError(\"No label column found for computing priors.\")\n",
        "    mapped = df[cand].apply(_to01)\n",
        "    return pd.DataFrame({\"Message ID\": df[\"Message ID\"].astype(int), \"label\": mapped.astype(int)})\n",
        "\n",
        "labels_df = _get_labels_df(raw_df)\n",
        "\n",
        "# Use only training split for priors\n",
        "train_mask   = labels_df[\"Message ID\"] % 30 != 0\n",
        "train_labels = labels_df.loc[train_mask].copy()\n",
        "n_docs = len(train_labels)\n",
        "n_ham  = (train_labels[\"label\"] == 0).sum()\n",
        "n_spam = (train_labels[\"label\"] == 1).sum()\n",
        "\n",
        "# Laplace-smoothed priors\n",
        "P_ham  = (n_ham + 1) / (n_docs + 2)\n",
        "P_spam = (n_spam + 1) / (n_docs + 2)\n",
        "log_prior_ham  = math.log(P_ham)\n",
        "log_prior_spam = math.log(P_spam)\n",
        "\n",
        "print(f\"Priors (from TRAIN split): P(ham)={P_ham:.4f}, P(spam)={P_spam:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score TEST rows -> posterior P(ham|msg), P(spam|msg)\n",
        "def _posterior_probs(feats: dict[str, int]) -> tuple[float, float]:\n",
        "    lh = log_prior_ham\n",
        "    ls = log_prior_spam\n",
        "    for tok, cnt in feats.items():\n",
        "        if cnt <= 0:\n",
        "            continue\n",
        "        if tok in vocab:\n",
        "            lh += cnt * log_ham[tok]\n",
        "            ls += cnt * log_spam[tok]\n",
        "        # OOV tokens are ignored; under Laplace smoothing they add the same constant to both classes.\n",
        "    # Normalize (log-sum-exp for 2 classes)\n",
        "    m = max(lh, ls)\n",
        "    ph = math.exp(lh - m)\n",
        "    ps = math.exp(ls - m)\n",
        "    z = ph + ps\n",
        "    return ph / z, ps / z\n",
        "\n",
        "ham_post, spam_post = [], []\n",
        "for feats in test_df[\"features_dict\"]:\n",
        "    ph, ps = _posterior_probs(feats)\n",
        "    ham_post.append(ph)\n",
        "    spam_post.append(ps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opc86JSEaAQM"
      },
      "source": [
        "Save your prediction probabilities in \"test-predictions.tsv\" with the same columns as \"train-predictions.tsv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qIg1XaY_Z_Rr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved -> test-predictions.tsv\n",
            " message id      ham          spam\n",
            "          0 0.054342  9.456579e-01\n",
            "         30 1.000000  5.596193e-81\n",
            "         60 1.000000  1.749893e-12\n",
            "         90 1.000000  1.826141e-34\n",
            "        120 1.000000 1.534749e-179\n",
            "        150 1.000000  6.981587e-12\n",
            "        180 0.999993  7.450849e-06\n",
            "        210 1.000000  1.527063e-42\n",
            "        240 1.000000  4.656409e-51\n",
            "        270 1.000000  3.272223e-39\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "out = pd.DataFrame({\n",
        "    \"message id\": test_df[\"Message ID\"].astype(int),\n",
        "    \"ham\": ham_post,\n",
        "    \"spam\": spam_post\n",
        "})\n",
        "out.to_csv(\"test-predictions.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(\"Saved -> test-predictions.tsv\")\n",
        "print(out.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLLbyE8paGqM"
      },
      "source": [
        "Submit \"test-predictions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU6ReUMsZNZ8"
      },
      "source": [
        "## Part 6: Construct ROC Curve\n",
        "\n",
        "For every probability threshold from 0.01 to .99 in increments of 0.01, compute the false and true positive rates from the test data using the spam class for positives.\n",
        "That is, if the predicted spam probability is greater than or equal to the threshold, predict spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QAx9jbDBYOVo"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "preds = pd.read_csv(\"test-predictions.tsv\", sep=\"\\t\")\n",
        "# Normalize column name to join later\n",
        "if \"message id\" not in preds.columns:\n",
        "    raise ValueError(\"test-predictions.tsv must have a 'message id' column.\")\n",
        "preds.rename(columns={\"message id\": \"Message ID\"}, inplace=True)\n",
        "\n",
        "if \"spam\" not in preds.columns:\n",
        "    raise ValueError(\"test-predictions.tsv must have a 'spam' probability column.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ground-truth labels and restrict to TEST split (Message ID % 30 == 0)\n",
        "def _load_original_df():\n",
        "    # Prefer in-memory if available\n",
        "    try:\n",
        "        return enron_spam_data.copy()\n",
        "    except NameError:\n",
        "        pass\n",
        "    # Try common filenames\n",
        "    for cand in [\"enron_spam_data.zip\", \"enron_spam_data.csv\", \"enron.csv\", \"data.csv\"]:\n",
        "        try:\n",
        "            return pd.read_csv(cand)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "raw_df = _load_original_df()\n",
        "if raw_df is None:\n",
        "    raise ValueError(\"Could not load the original dataset to obtain test labels.\")\n",
        "\n",
        "# Ensure Message ID exists\n",
        "if \"Message ID\" not in raw_df.columns:\n",
        "    for alt in [\"MessageID\", \"message_id\", \"msg_id\", \"Id\", \"id\"]:\n",
        "        if alt in raw_df.columns:\n",
        "            raw_df[\"Message ID\"] = raw_df[alt]\n",
        "            break\n",
        "if \"Message ID\" not in raw_df.columns:\n",
        "    raise ValueError(\"Could not find a 'Message ID' column in the original dataset.\")\n",
        "\n",
        "def _to01(x):\n",
        "    if isinstance(x, str):\n",
        "        xs = x.strip().lower()\n",
        "        if xs in {\"spam\",\"s\",\"1\",\"true\",\"yes\",\"y\"}: return 1\n",
        "        if xs in {\"ham\",\"h\",\"0\",\"false\",\"no\",\"n\",\"not_spam\",\"not spam\"}: return 0\n",
        "    if isinstance(x, (bool, np.bool_)): return 1 if x else 0\n",
        "    try:\n",
        "        v = int(x); return 1 if v != 0 else 0\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def _get_labels_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    preferred = [\n",
        "        \"Spam\",\"spam\",\"Label\",\"label\",\"Class\",\"class\",\"Target\",\"target\",\n",
        "        \"is_spam\",\"Is Spam\",\"Spam/Ham\",\"SpamHam\",\"Category\",\"category\"\n",
        "    ]\n",
        "    cand = next((c for c in preferred if c in df.columns), None)\n",
        "    if cand is None:\n",
        "        # auto-detect a binary-like column (exactly 2 unique non-null values)\n",
        "        for c in df.columns:\n",
        "            if c == \"Message ID\": continue\n",
        "            vals = pd.Series(df[c]).dropna().unique()\n",
        "            if len(vals) == 2:\n",
        "                cand = c; break\n",
        "    if cand is None:\n",
        "        raise ValueError(\"No label column found in the original dataset for ROC computation.\")\n",
        "    mapped = df[cand].apply(_to01).astype(int)\n",
        "    return pd.DataFrame({\"Message ID\": df[\"Message ID\"].astype(int), \"label\": mapped})\n",
        "\n",
        "labels = _get_labels_df(raw_df)\n",
        "\n",
        "# Keep only TEST split labels\n",
        "test_labels = labels.loc[(labels[\"Message ID\"] % 30) == 0].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge predictions with labels (test set)\n",
        "df = preds.merge(test_labels, on=\"Message ID\", how=\"inner\")\n",
        "if df.empty:\n",
        "    raise ValueError(\"No overlap between test predictions and test labels on 'Message ID'.\")\n",
        "y_true = df[\"label\"].astype(int).values\n",
        "p_spam = df[\"spam\"].astype(float).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Sweep thresholds and compute FPR/TPR\n",
        "#    Positive class = spam (1)\n",
        "#    Predict spam if p_spam >= threshold\n",
        "rows = []\n",
        "for i in range(1, 100):  # 0.01 .. 0.99\n",
        "    thr = i / 100.0\n",
        "    y_pred = (p_spam >= thr).astype(int)\n",
        "\n",
        "    # Confusion matrix components\n",
        "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
        "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
        "    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
        "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
        "\n",
        "    # Rates with safe division\n",
        "    tpr = tp / (tp + fn) if (tp + fn) > 0 else np.nan  # True Positive Rate (Recall)\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else np.nan  # False Positive Rate\n",
        "\n",
        "    rows.append((thr, fpr, tpr))\n",
        "\n",
        "roc = pd.DataFrame(rows, columns=[\"threshold\", \"false_positive_rate\", \"true_positive_rate\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baGaDOauX2vE"
      },
      "source": [
        "Save this data in a file \"roc.tsv\" with columns threshold, false_positive_rate and true_positive rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "eSHCzA85YP_I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 99 rows -> roc.tsv\n",
            " threshold  false_positive_rate  true_positive_rate\n",
            "      0.01             0.034420            0.991259\n",
            "      0.02             0.032609            0.991259\n",
            "      0.03             0.028986            0.989510\n",
            "      0.04             0.028986            0.989510\n",
            "      0.05             0.028986            0.989510\n",
            "      0.06             0.028986            0.989510\n",
            "      0.07             0.028986            0.989510\n",
            "      0.08             0.028986            0.989510\n",
            "      0.09             0.027174            0.989510\n",
            "      0.10             0.027174            0.989510\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "roc.to_csv(\"roc.tsv\", sep=\"\\t\", index=False)\n",
        "print(f\"Saved {len(roc)} rows -> roc.tsv\")\n",
        "print(roc.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4po8_NMYRuo"
      },
      "source": [
        "Submit \"roc.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynaBbiCZhMYi"
      },
      "source": [
        "## Part 7: Signup for Gemini API Key\n",
        "\n",
        "Create a free Gemini API key at https://aistudio.google.com/app/api-keys.\n",
        "You will need to do this with a personal Google account - it will not work with your BU Google account.\n",
        "This will not incur any charges unless you configure billing information for the key.\n",
        "\n",
        "You will be asked to start a Gemini free trial for week 11.\n",
        "This will not incur any charges unless you exceed expected usage by an order of magnitude.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3xFKcX6hxTL"
      },
      "source": [
        "No submission needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 8: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 9: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
